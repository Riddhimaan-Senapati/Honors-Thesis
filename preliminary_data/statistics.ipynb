{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1076797a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efb636b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: 'data/llm4eval_document_2024.jsonl'. Processing...\n",
      "\n",
      "--- Analysis Complete ---\n",
      "Total lines processed: 11621\n",
      "Number of unique docids found: 11621\n"
     ]
    }
   ],
   "source": [
    "jsonl_file_path = \"data/llm4eval_document_2024.jsonl\"\n",
    "# --- End Configuration ---\n",
    "\n",
    "# Check if the file exists (optional but good practice)\n",
    "if not os.path.exists(jsonl_file_path):\n",
    "    print(f\"Error: File not found at '{jsonl_file_path}'\")\n",
    "else:\n",
    "    print(f\"Found file: '{jsonl_file_path}'. Processing...\")\n",
    "    unique_docids = set()\n",
    "    line_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    try:\n",
    "        # Open the JSONL file for reading\n",
    "        with open(jsonl_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line_count += 1\n",
    "                # Strip leading/trailing whitespace\n",
    "                line = line.strip()\n",
    "                # Skip empty lines\n",
    "                if not line:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Parse the JSON string on the current line into a Python dict\n",
    "                    data = json.loads(line)\n",
    "\n",
    "                    # Extract the docid (use .get() for safety if key might be missing)\n",
    "                    docid = data.get(\"docid\")\n",
    "\n",
    "                    # Add the docid to the set if it exists\n",
    "                    # Sets automatically handle uniqueness\n",
    "                    if docid is not None:\n",
    "                        unique_docids.add(docid)\n",
    "                    else:\n",
    "                        # Optional: Warn if a line is missing 'docid'\n",
    "                        print(\n",
    "                            f\"Warning: 'docid' key not found in line {line_count}: {line[:100]}...\"\n",
    "                        )  # Print first 100 chars\n",
    "                        error_count += 1\n",
    "\n",
    "                except json.JSONDecodeError:\n",
    "                    # Handle lines that are not valid JSON\n",
    "                    print(\n",
    "                        f\"Error: Could not decode JSON on line {line_count}: {line[:100]}...\"\n",
    "                    )  # Print first 100 chars\n",
    "                    error_count += 1\n",
    "                except Exception as e:\n",
    "                    # Handle other potential errors during processing a line\n",
    "                    print(f\"Error processing line {line_count}: {e}\")\n",
    "                    error_count += 1\n",
    "\n",
    "        # Calculate the number of unique docids found\n",
    "        num_unique_docids = len(unique_docids)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"\\n--- Analysis Complete ---\")\n",
    "        print(f\"Total lines processed: {line_count}\")\n",
    "        print(f\"Number of unique docids found: {num_unique_docids}\")\n",
    "        if error_count > 0:\n",
    "            print(f\"Number of lines with errors or missing 'docid': {error_count}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # This case is handled by the initial check, but good to have redundancy\n",
    "        print(f\"Error: The file '{jsonl_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        # Handle potential errors during file opening/reading\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616db07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found file: 'data/llm4eval_dev_qrel_2024.txt'. Processing...\n",
      "\n",
      "--- Analysis Complete ---\n",
      "Total lines processed: 7263\n",
      "Total unique queries found: 25\n",
      "\n",
      "--- Unique Document Counts per Query ---\n",
      "Query 'q10': 614 unique documents\n",
      "Query 'q11': 270 unique documents\n",
      "Query 'q12': 259 unique documents\n",
      "Query 'q17': 403 unique documents\n",
      "Query 'q18': 374 unique documents\n",
      "Query 'q20': 188 unique documents\n",
      "Query 'q21': 352 unique documents\n",
      "Query 'q23': 345 unique documents\n",
      "Query 'q24': 203 unique documents\n",
      "Query 'q26': 282 unique documents\n",
      "Query 'q27': 359 unique documents\n",
      "Query 'q28': 362 unique documents\n",
      "Query 'q29': 166 unique documents\n",
      "Query 'q3': 201 unique documents\n",
      "Query 'q39': 169 unique documents\n",
      "Query 'q40': 231 unique documents\n",
      "Query 'q41': 215 unique documents\n",
      "Query 'q42': 337 unique documents\n",
      "Query 'q44': 214 unique documents\n",
      "Query 'q47': 170 unique documents\n",
      "Query 'q48': 217 unique documents\n",
      "Query 'q5': 543 unique documents\n",
      "Query 'q6': 232 unique documents\n",
      "Query 'q7': 224 unique documents\n",
      "Query 'q8': 333 unique documents\n"
     ]
    }
   ],
   "source": [
    "# calculate number of unique query ids in data/llm4eval_dev_qrel_2024.txt\n",
    "# Total of 50 query ids split between dev and test txt files with the relevance scores of dev being known to us\n",
    "input_txt_file_path = \"data/llm4eval_dev_qrel_2024.txt\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(input_txt_file_path):\n",
    "    print(f\"Error: File not found at '{input_txt_file_path}'\")\n",
    "else:\n",
    "    print(f\"Found file: '{input_txt_file_path}'. Processing...\")\n",
    "\n",
    "    # Use defaultdict(set) to automatically create a new set for unseen queries\n",
    "    # and store unique docids for each query\n",
    "    query_to_docids = defaultdict(set)\n",
    "    all_unique_queries = set()  # To count total unique queries easily\n",
    "\n",
    "    line_count = 0\n",
    "    error_count = 0\n",
    "\n",
    "    try:\n",
    "        with open(input_txt_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                line_count += 1\n",
    "                line = line.strip()  # Remove leading/trailing whitespace\n",
    "\n",
    "                if not line:  # Skip empty lines\n",
    "                    continue\n",
    "\n",
    "                parts = line.split()  # Split the line by spaces\n",
    "\n",
    "                # Expecting at least 3 parts: query_id, something, doc_id, ...\n",
    "                if len(parts) >= 3:\n",
    "                    query_id = parts[0]\n",
    "                    doc_id = parts[2]\n",
    "\n",
    "                    # Add query to the set of all unique queries\n",
    "                    all_unique_queries.add(query_id)\n",
    "\n",
    "                    # Add the doc_id to the set associated with this query_id\n",
    "                    # defaultdict handles creating the set if query_id is new\n",
    "                    query_to_docids[query_id].add(doc_id)\n",
    "\n",
    "                else:\n",
    "                    # Line doesn't have the expected format\n",
    "                    print(\n",
    "                        f\"Warning: Skipping malformed line {line_count}: {line[:100]}...\"\n",
    "                    )  # Show first 100 chars\n",
    "                    error_count += 1\n",
    "\n",
    "        # --- Analysis Results ---\n",
    "        total_unique_queries = len(all_unique_queries)  # Or len(query_to_docids)\n",
    "\n",
    "        print(\"\\n--- Analysis Complete ---\")\n",
    "        print(f\"Total lines processed: {line_count}\")\n",
    "        print(f\"Total unique queries found: {total_unique_queries}\")\n",
    "        if error_count > 0:\n",
    "            print(f\"Number of lines skipped due to formatting errors: {error_count}\")\n",
    "\n",
    "        print(\"\\n--- Unique Document Counts per Query ---\")\n",
    "        # Sort queries for consistent output (optional)\n",
    "        sorted_queries = sorted(query_to_docids.keys())\n",
    "\n",
    "        if not sorted_queries:\n",
    "            print(\"No valid query-document pairs found.\")\n",
    "        else:\n",
    "            for query_id in sorted_queries:\n",
    "                unique_doc_count = len(query_to_docids[query_id])\n",
    "                print(f\"Query '{query_id}': {unique_doc_count} unique documents\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        # Handled by the initial check, but good for robustness\n",
    "        print(f\"Error: The file '{input_txt_file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        # Catch other potential errors during file reading/processing\n",
    "        print(f\"An unexpected error occurred: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
